# Generic fallback compose template.
services:
  vllm:
    image: ${VLLM_IMAGE}
    container_name: ${CONTAINER_NAME:-vllm_server}
    gpus: ${GPUS:-all}
    ipc: ${IPC_MODE:-host}
    restart: ${RESTART_POLICY:-unless-stopped}
    ulimits:
      memlock: ${MEMLOCK_LIMIT:--1}
      stack: ${STACK_LIMIT:-67108864}
    ports:
      - "${VLLM_PORT:-8000}:${VLLM_CONTAINER_PORT:-8000}"
    volumes:
      - ./state/hf:/data/hf
      - ./artifacts:/artifacts
    environment:
      HF_HOME: ${HF_HOME:-/data/hf}
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
      CUDA_DEVICE_MAX_CONNECTIONS: "${CUDA_DEVICE_MAX_CONNECTIONS:-1}"
      CUBLAS_WORKSPACE_CONFIG: "${CUBLAS_WORKSPACE_CONFIG:-:4096:8}"
    command:
      - --model
      - ${MODEL_ID}
      - --revision
      - ${MODEL_REVISION}
      - --served-model-name
      - ${SERVED_MODEL_NAME}
      - --host
      - ${VLLM_HOST:-0.0.0.0}
      - --port
      - "${VLLM_CONTAINER_PORT:-8000}"
