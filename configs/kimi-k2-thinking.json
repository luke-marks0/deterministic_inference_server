{
  "integrity": {
    "enforce_on_wait": false,
    "expected_snapshot_manifest": "manifests/kimi-k2-thinking/{revision}.sha256"
  },
  "model": {
    "id": "moonshotai/Kimi-K2-Thinking",
    "locked_at_utc": "2026-02-17T00:00:00Z",
    "revision": "5b64d2a87e32f24d6231ef01ee676f65e54496a7",
    "served_name": "kimi-k2-thinking"
  },
  "profile": {
    "description": "Reference profile for Kimi-K2-Thinking with deterministic-oriented vLLM flags.",
    "id": "kimi-k2-thinking-int4"
  },
  "runtime": {
    "api_host": "0.0.0.0",
    "compose_project_name": "vllm_kimi_k2_thinking",
    "container_name": "vllm_kimi_k2_thinking",
    "container_port": 8000,
    "environment": {
      "CUBLAS_WORKSPACE_CONFIG": ":4096:8",
      "CUDA_DEVICE_MAX_CONNECTIONS": "1",
      "HF_HOME": "/data/hf",
      "PYTORCH_CUDA_ALLOC_CONF": "expandable_segments:True"
    },
    "bootstrap_pip_packages": [
      "blobfile==2.1.1"
    ],
    "gpus": "all",
    "host_port": 8001,
    "image": "vllm/vllm-openai:v0.11.0",
    "ipc_mode": "host",
    "memlock": -1,
    "paths": {
      "artifacts": "artifacts",
      "hf_cache": "state/hf"
    },
    "required_secret_env": [
    ],
    "restart": "unless-stopped",
    "stack": 67108864
  },
  "sampling_defaults": {
    "chunk_max_tokens": 1024,
    "seed": 424242,
    "target_tokens": 2000,
    "temperature": 0.0,
    "timeout_seconds": 600,
    "top_p": 0.95
  },
  "schema_version": 1,
  "smoke_test": {
    "max_tokens": 32,
    "prompt": "Write exactly one sentence that says the server is healthy.",
    "seed": 424242,
    "temperature": 0.0
  },
  "vllm": {
    "flags": [
      "--dtype=auto",
      "--tensor-parallel-size=8",
      "--pipeline-parallel-size=1",
      "--max-model-len=768",
      "--seed=424242",
      "--max-num-seqs=1",
      "--max-num-batched-tokens=256",
      "--gpu-memory-utilization=0.95",
      "--enforce-eager",
      "--trust-remote-code",
      "--enable-auto-tool-choice",
      "--tool-call-parser=kimi_k2",
      "--reasoning-parser=qwen3",
      "--disable-log-requests"
    ]
  }
}
